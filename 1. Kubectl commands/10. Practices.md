# Practice Exercises

This file groups all the practice exercises from the checkbox requirements into organized sections.

## [ ] Practice: Create/delete pods, scale deployments up/down

### Pod Creation and Deletion Practice

#### Exercise 1: Basic Pod Lifecycle
```bash
# 1. Create a simple pod
kubectl run practice-pod --image=nginx:latest

# 2. Verify pod is running
kubectl get pods
kubectl get pod practice-pod -o wide

# 3. Get detailed pod information
kubectl describe pod practice-pod

# 4. Test pod connectivity
kubectl exec practice-pod -- nginx -v

# 5. Delete the pod
kubectl delete pod practice-pod

# 6. Verify deletion
kubectl get pods
```

#### Exercise 2: Pod Creation with Labels
```bash
# 1. Create pod with labels
kubectl run labeled-pod --image=busybox --labels="app=test,environment=practice" -- sleep 3600

# 2. List pods with labels
kubectl get pods --show-labels

# 3. Filter pods by label
kubectl get pods -l app=test
kubectl get pods -l environment=practice

# 4. Delete pod using label selector
kubectl delete pods -l app=test

# 5. Verify deletion
kubectl get pods -l app=test
```

#### Exercise 3: Multiple Pod Management
```bash
# 1. Create multiple pods
kubectl run pod1 --image=nginx:latest
kubectl run pod2 --image=nginx:latest
kubectl run pod3 --image=busybox -- sleep 3600

# 2. List all pods
kubectl get pods

# 3. Delete specific pods
kubectl delete pod pod1 pod2

# 4. Delete remaining pod with force (if needed)
kubectl delete pod pod3 --force --grace-period=0

# 5. Verify all pods are deleted
kubectl get pods
```

### Deployment Scaling Practice

#### Exercise 1: Basic Deployment Scaling
```bash
# 1. Create deployment
kubectl create deployment web-app --image=nginx:1.21 --replicas=2

# 2. Check initial deployment
kubectl get deployments
kubectl get pods -l app=web-app

# 3. Scale up
kubectl scale deployment web-app --replicas=5

# 4. Monitor scaling
kubectl get pods -l app=web-app -w
# Press Ctrl+C to stop watching

# 5. Verify final state
kubectl get deployment web-app
kubectl get pods -l app=web-app

# 6. Scale down
kubectl scale deployment web-app --replicas=1

# 7. Clean up
kubectl delete deployment web-app
```

#### Exercise 2: Multiple Deployment Scaling
```bash
# 1. Create multiple deployments
kubectl create deployment frontend --image=nginx:1.21 --replicas=2
kubectl create deployment backend --image=httpd:2.4 --replicas=3

# 2. Check initial state
kubectl get deployments

# 3. Scale both deployments
kubectl scale deployment frontend backend --replicas=4

# 4. Verify scaling
kubectl get deployments
kubectl get pods -l app=frontend
kubectl get pods -l app=backend

# 5. Scale down to different values
kubectl scale deployment frontend --replicas=2
kubectl scale deployment backend --replicas=1

# 6. Clean up
kubectl delete deployment frontend backend
```

#### Exercise 3: Scaling with Monitoring
```bash
# 1. Create deployment
kubectl create deployment monitored-app --image=nginx:alpine --replicas=1

# 2. Set up monitoring in one terminal
kubectl get pods -l app=monitored-app -w &
WATCH_PID=$!

# 3. Scale up gradually
kubectl scale deployment monitored-app --replicas=3
sleep 10
kubectl scale deployment monitored-app --replicas=5
sleep 10
kubectl scale deployment monitored-app --replicas=2

# 4. Stop monitoring
kill $WATCH_PID

# 5. Check final state
kubectl get deployment monitored-app
kubectl describe deployment monitored-app

# 6. Clean up
kubectl delete deployment monitored-app
```

## [ ] Practice: Tail logs from running applications, troubleshoot failed pods

### Log Tailing Practice

#### Exercise 1: Real-time Log Monitoring
```bash
# 1. Create application that generates logs
kubectl run log-app --image=busybox -- /bin/sh -c "while true; do echo 'Log entry at '$(date); sleep 3; done"

# 2. Wait for pod to start
kubectl get pods -l run=log-app

# 3. Follow logs in real-time
kubectl logs log-app -f
# Watch logs for 30 seconds, then press Ctrl+C

# 4. Get last 10 log entries
kubectl logs log-app --tail=10

# 5. Get logs with timestamps
kubectl logs log-app --timestamps --tail=5

# 6. Clean up
kubectl delete pod log-app
```

#### Exercise 2: Multi-Container Log Practice
```bash
# 1. Create multi-container pod
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: multi-log-pod
spec:
  containers:
  - name: container1
    image: busybox
    command: ['/bin/sh', '-c', 'while true; do echo "Container1: $(date)"; sleep 2; done']
  - name: container2
    image: busybox
    command: ['/bin/sh', '-c', 'while true; do echo "Container2: $(date)"; sleep 3; done']
EOF

# 2. Wait for pod to be ready
kubectl get pod multi-log-pod

# 3. View logs from specific containers
kubectl logs multi-log-pod -c container1 --tail=5
kubectl logs multi-log-pod -c container2 --tail=5

# 4. Follow logs from all containers
kubectl logs multi-log-pod --all-containers=true -f --prefix=true
# Press Ctrl+C after observing logs

# 5. Clean up
kubectl delete pod multi-log-pod
```

### Troubleshooting Failed Pods Practice

#### Exercise 1: Image Pull Failure
```bash
# 1. Create pod with non-existent image
kubectl run failing-pod --image=nonexistent:image

# 2. Check pod status
kubectl get pods

# 3. Investigate the issue
kubectl describe pod failing-pod

# 4. Look for specific events
kubectl get events --sort-by=.metadata.creationTimestamp | grep failing-pod

# 5. Try to get logs (will fail)
kubectl logs failing-pod || echo "No logs available - pod not running"

# 6. Fix by using correct image
kubectl delete pod failing-pod
kubectl run fixed-pod --image=nginx:latest

# 7. Verify fix
kubectl get pods
kubectl describe pod fixed-pod

# 8. Clean up
kubectl delete pod fixed-pod
```

#### Exercise 2: Application Crash Troubleshooting
```bash
# 1. Create pod that will crash
kubectl run crash-pod --image=busybox -- /bin/sh -c "echo 'Starting...'; sleep 5; echo 'Crashing!'; exit 1"

# 2. Monitor pod status
kubectl get pods -w &
WATCH_PID=$!
sleep 15  # Let it crash and restart a few times
kill $WATCH_PID

# 3. Check pod status
kubectl get pod crash-pod

# 4. Get current logs
kubectl logs crash-pod

# 5. Get previous logs (from crashed container)
kubectl logs crash-pod --previous

# 6. Describe pod to see restart count and events
kubectl describe pod crash-pod

# 7. Clean up
kubectl delete pod crash-pod
```

#### Exercise 3: Resource Constraint Troubleshooting
```bash
# 1. Create pod with impossible resource requests
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "100Gi"  # Impossible request
        cpu: "50"        # Impossible request
EOF

# 2. Check pod status
kubectl get pods

# 3. Investigate why pod is pending
kubectl describe pod resource-pod

# 4. Look at events
kubectl get events --sort-by=.metadata.creationTimestamp | grep resource-pod

# 5. Fix the resource requests
kubectl delete pod resource-pod
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "64Mi"
        cpu: "100m"
EOF

# 6. Verify fix
kubectl get pod resource-pod
kubectl describe pod resource-pod

# 7. Clean up
kubectl delete pod resource-pod
```

## [ ] Practice: Update a ConfigMap and verify application picks up changes

### ConfigMap Update Practice

#### Exercise 1: Environment Variable vs Volume Mount
```bash
# 1. Create initial ConfigMap
kubectl create configmap practice-config \
  --from-literal=app_name="Original App" \
  --from-literal=version="1.0" \
  --from-literal=debug_mode="false"

# 2. Create pod with both env vars and volume mount
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: config-practice-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["/bin/sh", "-c"]
    args: ["while true; do echo '=== ENV VARS ==='; echo 'APP_NAME:'$APP_NAME; echo 'VERSION:'$VERSION; echo '=== FILES ==='; ls /etc/config/; cat /etc/config/app_name; echo; sleep 30; done"]
    env:
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: practice-config
          key: app_name
    - name: VERSION
      valueFrom:
        configMapKeyRef:
          name: practice-config
          key: version
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: practice-config
EOF

# 3. Wait for pod to start and check initial values
kubectl get pod config-practice-pod
kubectl logs config-practice-pod --tail=10

# 4. Update ConfigMap
kubectl patch configmap practice-config -p '{"data":{"app_name":"Updated App","version":"2.0"}}'

# 5. Check values immediately after update
kubectl logs config-practice-pod --tail=10
echo "Environment variables should still show old values"

# 6. Check file content (should update within ~1 minute)
kubectl exec config-practice-pod -- cat /etc/config/app_name

# 7. Wait and check again
echo "Waiting 60 seconds for volume mount to update..."
sleep 60
kubectl exec config-practice-pod -- cat /etc/config/app_name
kubectl logs config-practice-pod --tail=5

# 8. Restart pod to see env var changes
kubectl delete pod config-practice-pod
kubectl apply -f - << EOF
apiVersion: v1
kind: Pod
metadata:
  name: config-practice-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["/bin/sh", "-c"]
    args: ["echo 'After restart - APP_NAME:'$APP_NAME' VERSION:'$VERSION; sleep 300"]
    env:
    - name: APP_NAME
      valueFrom:
        configMapKeyRef:
          name: practice-config
          key: app_name
    - name: VERSION
      valueFrom:
        configMapKeyRef:
          name: practice-config
          key: version
EOF

# 9. Check updated environment variables
kubectl logs config-practice-pod

# 10. Clean up
kubectl delete pod config-practice-pod
kubectl delete configmap practice-config
```

#### Exercise 2: Deployment ConfigMap Update
```bash
# 1. Create ConfigMap for web application
kubectl create configmap web-config \
  --from-literal=server_name="Original Server" \
  --from-literal=port="8080" \
  --from-literal=log_level="info"

# 2. Create deployment using ConfigMap
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web
        image: nginx:alpine
        env:
        - name: SERVER_NAME
          valueFrom:
            configMapKeyRef:
              name: web-config
              key: server_name
        envFrom:
        - configMapRef:
            name: web-config
            prefix: CONFIG_
EOF

# 3. Check initial deployment
kubectl get deployment web-deployment
kubectl get pods -l app=web-app

# 4. Verify initial config in pods
kubectl exec deployment/web-deployment -- env | grep -E "(SERVER_NAME|CONFIG_)"

# 5. Update ConfigMap
kubectl patch configmap web-config -p '{"data":{"server_name":"Updated Server","log_level":"debug"}}'

# 6. Check if pods automatically updated (they won't)
kubectl exec deployment/web-deployment -- env | grep -E "(SERVER_NAME|CONFIG_)"

# 7. Restart deployment to pick up changes
kubectl rollout restart deployment/web-deployment

# 8. Wait for rollout to complete
kubectl rollout status deployment/web-deployment

# 9. Verify updated configuration
kubectl exec deployment/web-deployment -- env | grep -E "(SERVER_NAME|CONFIG_)"

# 10. Clean up
kubectl delete deployment web-deployment
kubectl delete configmap web-config
```

#### Exercise 3: Configuration File Update Practice
```bash
# 1. Create configuration file
cat > app.conf << EOF
[server]
listen_port = 8080
server_name = original-app
debug = false

[database]
host = localhost
port = 5432
name = mydb
EOF

# 2. Create ConfigMap from file
kubectl create configmap file-config --from-file=app.conf

# 3. Create pod that reads config file
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: file-config-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ["/bin/sh", "-c"]
    args: ["while true; do echo '=== CONFIG FILE ==='; cat /etc/config/app.conf; echo '=================='; sleep 30; done"]
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: file-config
EOF

# 4. Check initial configuration
kubectl logs file-config-pod --tail=15

# 5. Update configuration file and ConfigMap
cat > app.conf << EOF
[server]
listen_port = 9090
server_name = updated-app
debug = true

[database]
host = db.example.com
port = 5432
name = production_db
EOF

kubectl create configmap file-config --from-file=app.conf --dry-run=client -o yaml | kubectl apply -f -

# 6. Monitor for file update (should happen within ~1 minute)
echo "Watching for config file update..."
for i in {1..12}; do
  echo "Check $i/12 (every 10 seconds)..."
  kubectl exec file-config-pod -- grep "server_name = updated-app" /etc/config/app.conf && break
  sleep 10
done

# 7. Verify final configuration
kubectl logs file-config-pod --tail=15

# 8. Clean up
kubectl delete pod file-config-pod
kubectl delete configmap file-config
rm app.conf
```

## Practice Summary Checklist

After completing all exercises, you should have practiced:

### Pod Operations:
- [ ] Creating pods with `kubectl run`
- [ ] Listing pods with `kubectl get pods`
- [ ] Describing pods with `kubectl describe pod`
- [ ] Deleting pods with `kubectl delete pod`
- [ ] Working with pod labels and selectors

### Deployment Scaling:
- [ ] Creating deployments
- [ ] Scaling deployments up and down
- [ ] Monitoring scaling operations
- [ ] Managing multiple deployments

### Log Management:
- [ ] Following logs with `-f`
- [ ] Viewing logs from specific containers with `-c`
- [ ] Using `--tail` and `--timestamps`
- [ ] Troubleshooting pod failures

### ConfigMap Updates:
- [ ] Understanding environment variable behavior
- [ ] Understanding volume mount behavior
- [ ] Restarting pods/deployments after ConfigMap changes
- [ ] Monitoring configuration updates

## Next Steps

1. **Combine concepts**: Try exercises that combine multiple kubectl operations
2. **Explore automation**: Look into tools like Helm for configuration management
3. **Production practices**: Learn about monitoring and alerting for configuration changes
4. **Advanced topics**: Explore operators and custom resources for configuration management