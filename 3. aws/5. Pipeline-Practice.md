# 5. Pipeline Practice

## ğŸ¯ Learning Objectives
- [ ] Trigger pipelines manually and automatically
- [ ] Review and analyze build logs effectively
- [ ] Understand artifacts flow and storage
- [ ] Practice pipeline monitoring and management
- [ ] Master end-to-end pipeline operations

## ğŸš€ Triggering Pipelines

### Manual Pipeline Triggers
```
AWS Console Method:
1. Navigate to CodePipeline â†’ Pipelines
2. Select your pipeline
3. Click "Release change" button
4. Confirm the manual execution
5. Monitor the pipeline progress

Result: New execution starts immediately
```

### Automatic Pipeline Triggers
```
GitHub Webhook Trigger:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Developer    â”‚â”€â”€â”€â†’â”‚ GitHub      â”‚â”€â”€â”€â†’â”‚ CodePipeline â”‚
â”‚ pushes code  â”‚    â”‚ sends       â”‚    â”‚ starts       â”‚
â”‚ to main      â”‚    â”‚ webhook     â”‚    â”‚ execution    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

S3 Object Change Trigger:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ File upload  â”‚â”€â”€â”€â†’â”‚ S3 Event    â”‚â”€â”€â”€â†’â”‚ CodePipeline â”‚
â”‚ to S3 bucket â”‚    â”‚ Notificationâ”‚    â”‚ execution    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Scheduled Trigger (via CloudWatch Events):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CloudWatch   â”‚â”€â”€â”€â†’â”‚ EventBridge â”‚â”€â”€â”€â†’â”‚ CodePipeline â”‚
â”‚ Schedule     â”‚    â”‚ Rule        â”‚    â”‚ execution    â”‚
â”‚ (cron/rate)  â”‚    â”‚             â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trigger Configuration Examples
```json
// GitHub Webhook Configuration
{
  "ActionTypeId": {
    "Category": "Source",
    "Owner": "ThirdParty",
    "Provider": "GitHub",
    "Version": "1"
  },
  "Configuration": {
    "Owner": "myorganization",
    "Repo": "my-web-app",
    "Branch": "main",
    "OAuthToken": "{{resolve:secretsmanager:github-token}}"
  },
  "OutputArtifacts": [
    {
      "Name": "SourceOutput"
    }
  ]
}

// S3 Source Configuration
{
  "ActionTypeId": {
    "Category": "Source", 
    "Owner": "AWS",
    "Provider": "S3",
    "Version": "1"
  },
  "Configuration": {
    "S3Bucket": "my-source-bucket",
    "S3ObjectKey": "source/application.zip",
    "PollForSourceChanges": "false"
  }
}
```

## ğŸ“Š Monitoring Pipeline Execution

### Pipeline Status Dashboard
```
Pipeline Execution View:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ my-web-pipeline                   Execution #47     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Started: 2024-02-03 14:30:00 UTC                   â”‚
â”‚ Duration: 8m 32s                                    â”‚
â”‚ Triggered by: Webhook (GitHub)                      â”‚
â”‚ Source Version: abc123f "Fix login bug"            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Source  â”‚â†’â”‚ Build   â”‚â†’â”‚ Test    â”‚â†’â”‚ Deploy  â”‚    â”‚
â”‚ â”‚   âœ…    â”‚ â”‚   âœ…    â”‚ â”‚   ğŸ”„    â”‚ â”‚   â¸ï¸    â”‚    â”‚
â”‚ â”‚ 0m 15s  â”‚ â”‚ 5m 20s  â”‚ â”‚ 2m 45s  â”‚ â”‚ --:--   â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Status Indicators:
âœ… Succeeded  âŒ Failed  ğŸ”„ In Progress  â¸ï¸ Not Started  â­ï¸ Superseded
```

### Real-time Execution Monitoring
```bash
# AWS CLI to monitor pipeline execution
aws codepipeline get-pipeline-execution \
  --pipeline-name my-web-pipeline \
  --pipeline-execution-id a1b2c3d4-e5f6-7890-abcd-ef1234567890

# Get current pipeline state
aws codepipeline get-pipeline-state \
  --name my-web-pipeline

# List recent executions
aws codepipeline list-pipeline-executions \
  --pipeline-name my-web-pipeline \
  --max-items 5
```

### Stage-Level Monitoring
```
Detailed Stage View:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build Stage                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Action: CodeBuild-Build                     â”‚
â”‚ Status: Succeeded âœ…                        â”‚
â”‚ Started: 2024-02-03 14:30:15 UTC           â”‚
â”‚ Completed: 2024-02-03 14:35:35 UTC         â”‚
â”‚ Duration: 5m 20s                           â”‚
â”‚                                             â”‚
â”‚ Input Artifacts:                            â”‚
â”‚ â””â”€â”€ SourceOutput (from Source stage)       â”‚
â”‚                                             â”‚
â”‚ Output Artifacts:                           â”‚
â”‚ â””â”€â”€ BuildOutput (472 KB)                   â”‚
â”‚                                             â”‚
â”‚ [View details] [View logs] [Download logs] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Reviewing Build Logs Effectively

### Structured Log Review Process
```
1. Pipeline Overview:
   - Check overall execution status
   - Identify failed/successful stages
   - Note execution duration and triggers

2. Stage-Level Analysis:
   - Focus on failed stages first
   - Review stage duration patterns
   - Check input/output artifacts

3. Build Log Deep Dive:
   - Start with phase summary
   - Look for error patterns
   - Trace command execution flow

4. Root Cause Analysis:
   - Correlate errors with recent changes
   - Check external dependencies
   - Validate environment configurations
```

### Log Analysis Techniques
```bash
# Extract specific information from logs
grep -i "error\|failed\|exception" build.log > errors.txt
grep -A5 -B5 "npm ERR!" build.log > npm_errors.txt
grep "Duration:" build.log > phase_durations.txt

# Parse build timing information
awk '/Phase complete:/ {print $4, $6, $8}' build.log | sort

# Find longest running commands
grep "Running command" build.log | while read line; do
  echo "$line"
  # Extract timestamp and calculate duration
done
```

### Log Filtering and Search
```sql
-- CloudWatch Insights queries for build analysis

-- Find all failed builds today
fields @timestamp, @message
| filter @message like /FAILED/
| filter @timestamp > dateadd(hour, -24, now())
| sort @timestamp desc
| limit 20

-- Analyze build phase durations
fields @timestamp, @message
| filter @message like /Duration:/
| parse @message "Phase complete: * State: * Duration: *" as phase, state, duration
| filter state = "SUCCEEDED"
| stats avg(duration) by phase

-- Find builds with specific error patterns
fields @timestamp, @message
| filter @message like /npm ERR!/ or @message like /Command did not exit successfully/
| sort @timestamp desc
| limit 50
```

## ğŸ“¦ Understanding Artifacts

### Artifact Lifecycle
```
Artifact Flow in Pipeline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    artifact     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    artifact     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source  â”‚ SourceOutput   â”‚ Build   â”‚  BuildOutput   â”‚ Deploy  â”‚
â”‚ Stage   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ Stage   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ Stage   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                          â”‚                          â”‚
     â–¼                          â–¼                          â–¼
  Source Code              Built Application         Deployed App
  (Git Repo)               (JAR, ZIP, Docker)        (Running Service)
```

### Artifact Storage and Management
```
S3 Artifact Store Structure:
my-codepipeline-artifacts-bucket/
â”œâ”€â”€ my-web-pipeline/
â”‚   â”œâ”€â”€ SourceOutput/
â”‚   â”‚   â”œâ”€â”€ a1b2c3d4-e5f6/
â”‚   â”‚   â”‚   â””â”€â”€ source.zip
â”‚   â”‚   â””â”€â”€ b2c3d4e5-f6g7/
â”‚   â”‚       â””â”€â”€ source.zip
â”‚   â””â”€â”€ BuildOutput/
â”‚       â”œâ”€â”€ a1b2c3d4-e5f6/
â”‚       â”‚   â””â”€â”€ application.zip
â”‚       â””â”€â”€ b2c3d4e5-f6g7/
â”‚           â””â”€â”€ application.zip
â””â”€â”€ api-pipeline/
    â”œâ”€â”€ SourceOutput/
    â””â”€â”€ BuildOutput/

Artifact Properties:
- Encrypted at rest (KMS)
- Versioned by execution ID
- Automatic cleanup (retention policy)
- Cross-region replication available
```

### Artifact Inspection
```bash
# Download artifact for inspection
aws s3 cp s3://my-codepipeline-artifacts/my-pipeline/BuildOutput/a1b2c3d4-e5f6/artifact.zip ./

# Extract and examine contents
unzip artifact.zip -d artifact-contents/
find artifact-contents/ -type f | head -20

# Check artifact metadata
aws codepipeline get-pipeline-execution \
  --pipeline-name my-pipeline \
  --pipeline-execution-id a1b2c3d4-e5f6 \
  --query 'pipelineExecution.artifactRevisions'
```

### Artifact Configuration Examples
```yaml
# buildspec.yml artifact configuration
artifacts:
  files:
    - '**/*'
  base-directory: dist
  name: myapp-$CODEBUILD_BUILD_NUMBER
  secondary-artifacts:
    test-reports:
      files:
        - 'test-results/**/*'
      base-directory: reports
      name: test-reports-$CODEBUILD_BUILD_NUMBER
    documentation:
      files:
        - 'docs/**/*'
        - 'README.md'
        - 'CHANGELOG.md'
      name: documentation
```

## ğŸ› ï¸ Hands-On Practice Scenarios

### Scenario 1: Basic Pipeline Operation
```
Objective: Trigger and monitor a simple web application pipeline

Steps:
1. Fork the sample repository:
   https://github.com/aws-samples/codepipeline-demo

2. Create CodePipeline with:
   - Source: GitHub (your fork)
   - Build: CodeBuild (Node.js)
   - Deploy: S3 (static hosting)

3. Make a code change and push to trigger:
   - Update index.html title
   - Add console.log statement
   - Modify package.json version

4. Monitor execution:
   - Watch stage progression
   - Review build logs
   - Verify artifact creation
   - Test deployed application
```

### Scenario 2: Build Failure Investigation
```
Objective: Intentionally break the build and practice troubleshooting

Steps:
1. Introduce build errors:
   # In package.json
   {
     "scripts": {
       "build": "webpack --mode=production",
       "test": "jest --nonexistent-flag"  // Intentional error
     }
   }

2. Commit and push changes

3. Monitor pipeline execution:
   - Identify failed stage
   - Access build logs
   - Locate specific error messages
   - Understand root cause

4. Fix the issue:
   - Correct the package.json
   - Push fix
   - Monitor successful execution

5. Document findings:
   - Error pattern recognition
   - Log analysis techniques
   - Resolution steps
```

### Scenario 3: Multi-Environment Deployment
```
Objective: Set up and manage multi-environment pipeline

Pipeline Structure:
Source â†’ Build â†’ Deploy-Dev â†’ Manual-Approval â†’ Deploy-Staging â†’ Deploy-Prod

Practice Tasks:
1. Create environment-specific configurations
2. Implement manual approval gates
3. Monitor deployments across environments
4. Practice rollback procedures
5. Analyze artifact promotion flow

Environment Configurations:
# dev-config.json
{
  "environment": "development",
  "apiUrl": "https://dev-api.example.com",
  "debugMode": true,
  "replicas": 1
}

# prod-config.json
{
  "environment": "production", 
  "apiUrl": "https://api.example.com",
  "debugMode": false,
  "replicas": 3
}
```

### Scenario 4: Performance Optimization
```
Objective: Optimize pipeline performance through analysis

Optimization Areas:
1. Build Time Analysis:
   - Measure phase durations
   - Identify bottlenecks
   - Implement caching strategies
   - Parallel processing where possible

2. Artifact Size Optimization:
   - Analyze artifact contents
   - Remove unnecessary files
   - Implement compression
   - Use .gitignore patterns

3. Dependency Management:
   - Cache node_modules/dependencies
   - Use npm ci instead of npm install
   - Implement layer caching for Docker

Implementation:
# Optimized buildspec.yml
version: 0.2

phases:
  install:
    runtime-versions:
      nodejs: 18
    commands:
      - echo "Installing dependencies with cache"
      - npm ci --only=production --silent

cache:
  paths:
    - '/root/.npm/**/*'
    - 'node_modules/**/*'

artifacts:
  files:
    - '**/*'
    - '!node_modules/**/*'
    - '!src/**/*'
    - '!tests/**/*'
  base-directory: dist
```

## ğŸ“Š Pipeline Metrics and Analysis

### Key Performance Indicators
```
Pipeline Metrics to Track:
- Execution frequency (builds per day)
- Success rate (% successful builds)
- Mean time to deployment (MTTD)
- Build duration trends
- Failure recovery time
- Artifact size trends

CloudWatch Metrics Available:
- Pipeline execution duration
- Stage execution duration  
- Pipeline execution success/failure
- Stage action success/failure
```

### Custom Metrics Implementation
```python
# Lambda function to collect custom pipeline metrics
import boto3
import json
from datetime import datetime

def lambda_handler(event, context):
    cloudwatch = boto3.client('cloudwatch')
    
    # Extract pipeline information from event
    detail = event['detail']
    pipeline_name = detail['pipeline']
    execution_id = detail['execution-id']
    state = detail['state']
    
    # Calculate execution duration
    if state == 'SUCCEEDED' or state == 'FAILED':
        start_time = datetime.fromisoformat(detail['start-time'])
        end_time = datetime.fromisoformat(detail['end-time'])
        duration = (end_time - start_time).total_seconds()
        
        # Send custom metrics to CloudWatch
        cloudwatch.put_metric_data(
            Namespace='Custom/CodePipeline',
            MetricData=[
                {
                    'MetricName': 'ExecutionDuration',
                    'Dimensions': [
                        {
                            'Name': 'PipelineName',
                            'Value': pipeline_name
                        },
                        {
                            'Name': 'State',
                            'Value': state
                        }
                    ],
                    'Value': duration,
                    'Unit': 'Seconds'
                }
            ]
        )
```

## ğŸ“ Practice Exercises

### Exercise 1: Pipeline Triggering
1. **Set up automatic triggers** for your pipeline
2. **Test webhook functionality** by making code changes
3. **Practice manual triggering** from console
4. **Monitor trigger sources** and execution patterns

### Exercise 2: Log Analysis Mastery
1. **Create build failures** intentionally
2. **Navigate logs efficiently** to find root causes
3. **Use CloudWatch Insights** for log analysis
4. **Build troubleshooting playbooks**

### Exercise 3: Artifact Management
1. **Examine artifact contents** from different stages
2. **Download and inspect** build outputs
3. **Optimize artifact sizes** and contents
4. **Track artifact flow** through pipeline stages

### Exercise 4: Performance Monitoring
1. **Measure baseline performance** of your pipeline
2. **Implement optimization strategies**
3. **Compare before/after metrics**
4. **Document performance improvements**

## ğŸ“ Best Practices for Pipeline Operations

### Monitoring and Alerting
- âœ… Set up CloudWatch alarms for pipeline failures
- âœ… Create SNS notifications for critical events
- âœ… Implement dashboard for team visibility
- âœ… Monitor trend metrics for performance insights
- âœ… Configure escalation procedures

### Artifact Management
- âœ… Implement proper artifact naming conventions
- âœ… Set appropriate retention policies
- âœ… Minimize artifact sizes for efficiency
- âœ… Secure artifact storage with encryption
- âœ… Document artifact contents and purposes

### Operational Excellence
- âœ… Regularly review and optimize pipelines
- âœ… Document troubleshooting procedures
- âœ… Train team members on pipeline operations
- âœ… Implement automated testing of pipeline changes
- âœ… Maintain backup and recovery procedures

### Security Considerations
- âœ… Use IAM roles with least privilege principles
- âœ… Secure sensitive information in Parameter Store/Secrets Manager
- âœ… Enable CloudTrail logging for audit trails
- âœ… Regularly rotate access keys and tokens
- âœ… Implement resource-based policies where appropriate

## â¡ï¸ Next Steps
Now that you've practiced pipeline operations, proceed to [6. CloudWatch-Log-Groups-Streams.md](6.%20CloudWatch-Log-Groups-Streams.md) to learn about navigating CloudWatch logs and understanding how FluentBit forwards logs to CloudWatch.