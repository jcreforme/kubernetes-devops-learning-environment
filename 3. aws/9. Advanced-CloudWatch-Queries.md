# 9. Advanced CloudWatch Queries & Log Analysis

## üéØ Learning Objectives
- [ ] Master advanced CloudWatch Insights query patterns
- [ ] Implement error filtering and log aggregation strategies
- [ ] Create performance monitoring queries
- [ ] Build operational dashboards from log data
- [ ] Troubleshoot complex application issues through log analysis
- [ ] Optimize query performance and cost

## üìã Advanced Query Patterns

### Complex Error Filtering
```sql
-- Multi-layer error analysis
fields @timestamp, @message, kubernetes.namespace_name, kubernetes.pod_name
| filter @message like /ERROR|FATAL|Exception/
| parse @message /(?<error_type>\w+Error|Exception): (?<error_message>.*)/
| stats count() as error_count by error_type, kubernetes.namespace_name
| sort error_count desc
| limit 20

-- Error correlation across services
fields @timestamp, @message, kubernetes.labels.app, @requestId
| filter @message like /ERROR/ and ispresent(@requestId)
| sort @timestamp desc
| limit 50

-- Error patterns with context
fields @timestamp, @logStream, @message
| filter @message like /ERROR/
| parse @message /\[(?<timestamp>[^\]]+)\] (?<level>\w+) (?<logger>[^\s]+) - (?<msg>.*)/
| parse @logStream /(?<service>[^-]+)-(?<instance>[^-]+)-(?<hash>.*)/
| stats count() as errors, 
        earliest(@timestamp) as first_seen,
        latest(@timestamp) as last_seen
  by service, msg
| sort errors desc
```

### Performance Analysis Queries
```sql
-- Response time analysis with percentiles
fields @timestamp, @duration, @requestId, request.path
| filter ispresent(@duration) and @duration > 0
| stats count() as requests,
        avg(@duration) as avg_duration,
        min(@duration) as min_duration,
        max(@duration) as max_duration,
        pct(@duration, 50) as p50,
        pct(@duration, 90) as p90,
        pct(@duration, 95) as p95,
        pct(@duration, 99) as p99
  by request.path
| sort avg_duration desc

-- Slow query detection with stack traces
fields @timestamp, @duration, @message, @requestId
| filter @duration > 10000 or @message like /timeout/
| parse @message /Query: (?<query>.*?) Duration: (?<query_duration>\d+)ms/
| sort @duration desc
| limit 25

-- Resource utilization trends
fields @timestamp, @memoryUsed, @maxMemoryUsed, kubernetes.pod_name
| filter ispresent(@memoryUsed)
| stats avg(@memoryUsed) as avg_memory,
        max(@maxMemoryUsed) as peak_memory,
        count_distinct(kubernetes.pod_name) as active_pods
  by bin(5m)
| eval memory_utilization = avg_memory / peak_memory * 100
| sort @timestamp desc
```

### Business Logic Monitoring
```sql
-- User journey tracking
fields @timestamp, userId, action, status, @requestId
| filter ispresent(userId) and ispresent(action)
| sort @timestamp asc
| stats count() as action_count,
        earliest(@timestamp) as session_start,
        latest(@timestamp) as session_end
  by userId, @requestId
| eval session_duration = (session_end - session_start) / 1000
| sort session_duration desc

-- Transaction flow analysis
fields @timestamp, transaction_id, step, status, duration_ms
| filter ispresent(transaction_id)
| sort @timestamp asc
| stats count() as steps,
        sum(duration_ms) as total_duration,
        count(status="SUCCESS") as successful_steps,
        count(status="FAILED") as failed_steps
  by transaction_id
| eval success_rate = successful_steps / steps * 100
| filter success_rate < 100
| sort total_duration desc

-- Feature usage analytics
fields @timestamp, feature, user_id, action_type
| filter ispresent(feature) and action_type = "feature_usage"
| stats count() as usage_count,
        count_distinct(user_id) as unique_users
  by feature, bin(1h)
| sort usage_count desc
```

## üîç Advanced Log Aggregation

### Log Volume Analysis
```sql
-- Log volume by service over time
fields @timestamp, kubernetes.labels.app
| stats count() as log_count by kubernetes.labels.app, bin(15m)
| sort @timestamp desc, log_count desc

-- Log level distribution analysis  
fields @timestamp, level, kubernetes.namespace_name
| filter ispresent(level)
| stats count() as log_count by level, kubernetes.namespace_name, bin(1h)
| eval log_percentage = log_count / sum(log_count) * 100
| sort @timestamp desc, log_count desc

-- Error rate calculation
fields @timestamp, level, kubernetes.labels.app
| stats count() as total_logs,
        count(level="ERROR") as error_logs,
        count(level="WARN") as warn_logs
  by kubernetes.labels.app, bin(5m)
| eval error_rate = error_logs / total_logs * 100,
       warn_rate = warn_logs / total_logs * 100
| filter error_rate > 5 or warn_rate > 20
| sort error_rate desc
```

### Cross-Service Correlation
```sql
-- Request tracing across microservices
fields @timestamp, @message, trace_id, service_name, span_id
| filter ispresent(trace_id)
| parse @message /(?<operation>\w+) (?<resource>\/\w+) - (?<status_code>\d+) - (?<duration>\d+)ms/
| sort @timestamp asc
| stats count() as spans,
        sum(duration) as total_duration,
        earliest(@timestamp) as request_start,
        latest(@timestamp) as request_end
  by trace_id
| eval request_duration = (request_end - request_start) / 1000
| filter request_duration > 10
| sort request_duration desc

-- Service dependency mapping
fields @timestamp, source_service, target_service, operation, status_code
| filter ispresent(source_service) and ispresent(target_service)
| stats count() as call_count,
        count(status_code >= 400) as error_count,
        avg(duration) as avg_response_time
  by source_service, target_service, operation
| eval error_rate = error_count / call_count * 100
| sort error_rate desc, call_count desc

-- Circuit breaker monitoring
fields @timestamp, @message, service_name, circuit_state
| filter @message like /Circuit.*breaker/
| parse @message /Circuit breaker (?<state>OPEN|CLOSED|HALF_OPEN) for service (?<target_service>\w+)/
| stats count() as state_changes by service_name, target_service, state, bin(10m)
| sort @timestamp desc
```

## üìä Performance Monitoring Queries

### Application Performance Monitoring (APM)
```sql
-- Apdex score calculation (Application Performance Index)
fields @timestamp, @duration, @requestId
| filter ispresent(@duration)
| eval satisfied = if(@duration <= 500, 1, 0),
       tolerating = if(@duration > 500 and @duration <= 2000, 1, 0),
       frustrated = if(@duration > 2000, 1, 0)
| stats sum(satisfied) as satisfied_requests,
        sum(tolerating) as tolerating_requests, 
        sum(frustrated) as frustrated_requests,
        count() as total_requests
  by bin(15m)
| eval apdex = (satisfied_requests + (tolerating_requests / 2)) / total_requests
| sort @timestamp desc

-- SLA monitoring with uptime calculation
fields @timestamp, status_code, @duration
| filter ispresent(status_code)
| eval is_success = if(status_code < 400, 1, 0),
       is_available = if(status_code != 503 and status_code != 504, 1, 0),
       is_fast = if(@duration < 5000, 1, 0)
| stats sum(is_success) as successful_requests,
        sum(is_available) as available_requests,
        sum(is_fast) as fast_requests,
        count() as total_requests
  by bin(1h)
| eval success_rate = successful_requests / total_requests * 100,
       availability = available_requests / total_requests * 100,
       performance_rate = fast_requests / total_requests * 100
| sort @timestamp desc

-- Database performance monitoring
fields @timestamp, @message, query_type, duration_ms, rows_affected
| filter @message like /SQL/ or @message like /Database/
| parse @message /(?<query_type>SELECT|INSERT|UPDATE|DELETE).*duration: (?<duration>\d+)ms.*rows: (?<rows>\d+)/
| stats count() as query_count,
        avg(duration) as avg_duration,
        max(duration) as max_duration,
        sum(rows) as total_rows
  by query_type, bin(10m)
| sort avg_duration desc
```

### Infrastructure Monitoring
```sql
-- Container resource utilization
fields @timestamp, kubernetes.pod_name, cpu_percent, memory_percent, disk_io
| filter ispresent(cpu_percent) and ispresent(memory_percent)
| stats avg(cpu_percent) as avg_cpu,
        max(cpu_percent) as peak_cpu,
        avg(memory_percent) as avg_memory,
        max(memory_percent) as peak_memory
  by kubernetes.pod_name, bin(5m)
| filter peak_cpu > 80 or peak_memory > 85
| sort peak_cpu desc

-- Network latency analysis
fields @timestamp, source_ip, destination_ip, latency_ms, packet_loss
| filter ispresent(latency_ms)
| stats avg(latency_ms) as avg_latency,
        pct(latency_ms, 95) as p95_latency,
        max(latency_ms) as max_latency,
        avg(packet_loss) as avg_packet_loss
  by source_ip, destination_ip, bin(1m)
| filter avg_latency > 100 or avg_packet_loss > 0.1
| sort avg_latency desc

-- Kubernetes events correlation
fields @timestamp, @message, kubernetes.namespace_name, kubernetes.pod_name, reason
| filter @message like /Warning|Error/ and ispresent(reason)
| parse @message /(?<event_type>Warning|Error) (?<event_reason>\w+): (?<event_message>.*)/
| stats count() as event_count by kubernetes.namespace_name, event_reason, bin(10m)
| sort event_count desc
```

## üõ†Ô∏è Troubleshooting Patterns

### Error Cascade Analysis
```sql
-- Identify error propagation patterns
fields @timestamp, @message, service_name, @requestId, parent_request_id
| filter @message like /ERROR/
| sort @timestamp asc
| stats count() as error_count,
        earliest(@timestamp) as first_error,
        latest(@timestamp) as last_error
  by @requestId, service_name
| eval error_duration = (last_error - first_error) / 1000
| sort first_error asc

-- Root cause analysis through error correlation
fields @timestamp, @message, level, logger, @requestId
| filter level = "ERROR" and ispresent(@requestId)
| sort @timestamp asc
| stats count() as related_errors,
        earliest(@timestamp) as root_error_time,
        collect(logger) as affected_loggers,
        collect(@message) as error_messages
  by @requestId
| sort root_error_time asc

-- Dependency failure impact analysis
fields @timestamp, @message, upstream_service, downstream_service, status
| filter @message like /timeout|connection.*failed|service.*unavailable/
| parse @message /(?<failure_type>timeout|connection failed|service unavailable).*service: (?<failing_service>\w+)/
| stats count() as failure_count by failing_service, bin(5m)
| sort failure_count desc
```

### Performance Degradation Detection
```sql
-- Anomaly detection for response times
fields @timestamp, @duration, endpoint
| filter ispresent(@duration) and ispresent(endpoint)
| stats avg(@duration) as current_avg,
        pct(@duration, 95) as current_p95
  by endpoint, bin(5m)
| sort @timestamp desc
| limit 100

-- Memory leak detection
fields @timestamp, @memoryUsed, kubernetes.pod_name
| filter ispresent(@memoryUsed)
| sort @timestamp asc
| stats first(@memoryUsed) as initial_memory,
        last(@memoryUsed) as final_memory,
        max(@memoryUsed) as peak_memory,
        avg(@memoryUsed) as avg_memory
  by kubernetes.pod_name
| eval memory_growth = final_memory - initial_memory,
       growth_percentage = (final_memory - initial_memory) / initial_memory * 100
| filter growth_percentage > 50
| sort growth_percentage desc

-- Connection pool exhaustion
fields @timestamp, @message, pool_name, active_connections, max_connections
| filter @message like /connection.*pool/
| parse @message /Pool: (?<pool_name>\w+) Active: (?<active>\d+) Max: (?<max>\d+)/
| eval pool_utilization = active / max * 100
| stats avg(pool_utilization) as avg_utilization,
        max(pool_utilization) as peak_utilization
  by pool_name, bin(1m)
| filter peak_utilization > 90
| sort peak_utilization desc
```

## üìà Custom Metrics Creation

### Business Metrics from Logs
```sql
-- Revenue calculation from transaction logs
fields @timestamp, @message, transaction_amount, currency, user_id
| filter @message like /transaction.*completed/
| parse @message /amount: (?<amount>\d+\.\d+) (?<curr>\w+)/
| filter curr = "USD"
| stats sum(amount) as total_revenue,
        count() as transaction_count,
        count_distinct(user_id) as unique_customers,
        avg(amount) as avg_transaction_value
  by bin(1h)
| sort @timestamp desc

-- User engagement metrics
fields @timestamp, user_id, page_view, session_duration, feature_used
| filter ispresent(user_id)
| stats count(page_view) as page_views,
        sum(session_duration) as total_session_time,
        count_distinct(feature_used) as features_used,
        count() as total_actions
  by user_id, bin(1d)
| eval avg_session_duration = total_session_time / total_actions,
       engagement_score = (page_views * 0.3) + (features_used * 0.7)
| sort engagement_score desc

-- Conversion funnel analysis
fields @timestamp, user_id, funnel_step, action_type
| filter ispresent(funnel_step) and action_type = "step_completed"
| stats count() as completions by funnel_step, bin(1h)
| sort @timestamp desc, funnel_step asc
```

### Operational Metrics
```sql
-- Deployment success rate
fields @timestamp, @message, deployment_id, environment, status
| filter @message like /deployment/
| parse @message /deployment (?<deploy_id>\w+) (?<env>\w+) (?<result>SUCCESS|FAILED)/
| stats count() as total_deployments,
        count(result="SUCCESS") as successful_deployments
  by env, bin(1d)
| eval success_rate = successful_deployments / total_deployments * 100
| sort success_rate asc

-- Feature flag usage tracking
fields @timestamp, feature_flag, user_id, flag_value, experiment_group
| filter ispresent(feature_flag)
| stats count() as total_evaluations,
        count(flag_value="true") as enabled_evaluations,
        count_distinct(user_id) as unique_users
  by feature_flag, experiment_group, bin(1h)
| eval adoption_rate = enabled_evaluations / total_evaluations * 100
| sort adoption_rate desc

-- Cache performance metrics
fields @timestamp, @message, cache_key, cache_hit, response_time
| filter @message like /cache/
| stats count() as total_requests,
        count(cache_hit="true") as cache_hits,
        avg(response_time) as avg_response_time
  by bin(10m)
| eval cache_hit_rate = cache_hits / total_requests * 100
| sort @timestamp desc
```

## üîß Query Optimization Strategies

### Performance Optimization
```sql
-- Optimized query structure (DO)
fields @timestamp, level, message
| filter @timestamp >= dateadd(hour, -1, now())  -- Time filter first
| filter level = "ERROR"                         -- Specific filters early
| parse message /Error: (?<error_type>\w+)/      -- Parse only needed fields
| stats count() by error_type                    -- Aggregate
| limit 20                                       -- Limit results

-- Inefficient query structure (DON'T)
fields @timestamp, @message, kubernetes.pod_name, kubernetes.namespace_name, level
| parse @message /.*Error: (?<error_type>\w+).*/  -- Parse everything first
| filter level = "ERROR"                          -- Filter after parsing
| filter @timestamp >= dateadd(hour, -1, now())  -- Time filter last
| stats count() by error_type, kubernetes.pod_name, kubernetes.namespace_name
```

### Cost Optimization
```sql
-- Data volume reduction techniques
fields @timestamp, level, brief_message
| filter @timestamp >= dateadd(hour, -2, now())  -- Narrow time window
| filter level in ["ERROR", "WARN"]              -- Specific log levels only
| parse @message /^(?<brief_message>.{0,100})/   -- Truncate long messages
| dedup brief_message                             -- Remove duplicates
| limit 100                                      -- Restrict results

-- Efficient aggregation patterns
fields @timestamp, kubernetes.labels.app, level
| filter @timestamp >= dateadd(minute, -30, now())
| filter level in ["ERROR", "FATAL"]
| stats count() as error_count by kubernetes.labels.app, bin(5m)
| sort error_count desc
| limit 50
```

### Query Template Library
```sql
-- Template: Error Rate Monitoring
fields @timestamp, level, kubernetes.labels.app
| filter @timestamp >= dateadd(${TIME_RANGE}, now())
| filter kubernetes.labels.app = "${SERVICE_NAME}"
| stats count() as total,
        count(level="ERROR") as errors
  by bin(${BIN_SIZE})
| eval error_rate = errors / total * 100
| sort @timestamp desc

-- Template: Performance Baseline
fields @timestamp, @duration, endpoint
| filter @timestamp >= dateadd(${TIME_RANGE}, now())
| filter endpoint like /${ENDPOINT_PATTERN}/
| stats count() as requests,
        avg(@duration) as avg_duration,
        pct(@duration, ${PERCENTILE}) as p${PERCENTILE}
  by bin(${BIN_SIZE})
| sort @timestamp desc

-- Template: User Activity Analysis
fields @timestamp, ${USER_FIELD}, ${ACTION_FIELD}
| filter @timestamp >= dateadd(${TIME_RANGE}, now())
| filter ${ACTION_FIELD} in [${ACTION_LIST}]
| stats count() as activity_count,
        count_distinct(${USER_FIELD}) as unique_users
  by ${ACTION_FIELD}, bin(${BIN_SIZE})
| sort activity_count desc
```

## üìä Building Operational Dashboards

### Dashboard Query Patterns
```sql
-- Widget 1: Error Rate Over Time
fields @timestamp, level
| filter @timestamp >= dateadd(hour, -24, now())
| stats count() as total_logs,
        count(level="ERROR") as error_logs
  by bin(1h)
| eval error_rate = error_logs / total_logs * 100
| sort @timestamp desc

-- Widget 2: Top Error Messages
fields @message, kubernetes.labels.app
| filter @timestamp >= dateadd(hour, -4, now())
| filter @message like /ERROR/
| stats count() as error_count by @message, kubernetes.labels.app
| sort error_count desc
| limit 10

-- Widget 3: Service Health Status
fields @timestamp, kubernetes.labels.app, level
| filter @timestamp >= dateadd(minute, -15, now())
| stats count() as total_logs,
        count(level="ERROR") as errors,
        count(level="WARN") as warnings
  by kubernetes.labels.app
| eval health_score = (total_logs - errors - warnings) / total_logs * 100
| sort health_score asc

-- Widget 4: Response Time Trends
fields @timestamp, @duration, endpoint
| filter @timestamp >= dateadd(hour, -6, now())
| filter ispresent(@duration)
| stats avg(@duration) as avg_response_time,
        pct(@duration, 95) as p95_response_time
  by endpoint, bin(30m)
| sort @timestamp desc
```

### Alert Query Patterns
```sql
-- High Error Rate Alert
fields @timestamp, level
| filter @timestamp >= dateadd(minute, -5, now())
| stats count() as total, count(level="ERROR") as errors
| eval error_rate = errors / total * 100
| filter error_rate > 10

-- Slow Response Alert
fields @timestamp, @duration
| filter @timestamp >= dateadd(minute, -10, now())
| filter ispresent(@duration)
| stats pct(@duration, 95) as p95_duration
| filter p95_duration > 5000

-- Memory Usage Alert
fields @timestamp, @memoryUsed, kubernetes.pod_name
| filter @timestamp >= dateadd(minute, -5, now())
| filter ispresent(@memoryUsed)
| stats max(@memoryUsed) as peak_memory by kubernetes.pod_name
| filter peak_memory > 1000000000  -- 1GB
```

## üìù Practice Exercises

### Exercise 1: Error Analysis
1. **Create queries** to identify error patterns across services
2. **Build correlation** between errors and performance degradation
3. **Implement root cause** analysis workflows
4. **Set up automated** error detection alerts

### Exercise 2: Performance Monitoring
1. **Develop performance** baseline queries
2. **Create SLA monitoring** dashboards
3. **Implement anomaly** detection for response times
4. **Build capacity planning** queries

### Exercise 3: Business Intelligence
1. **Extract business metrics** from application logs
2. **Create user behavior** analysis queries
3. **Build conversion funnel** tracking
4. **Implement revenue** monitoring dashboards

### Exercise 4: Cost Optimization
1. **Optimize existing queries** for performance
2. **Implement query templates** for reusability
3. **Set up cost monitoring** for log analysis
4. **Create efficient** aggregation patterns

## üéØ Advanced Scenarios

### Multi-Region Log Analysis
```sql
-- Cross-region performance comparison
fields @timestamp, region, @duration, endpoint
| filter ispresent(region) and ispresent(@duration)
| stats avg(@duration) as avg_response_time,
        count() as request_count
  by region, endpoint, bin(1h)
| sort @timestamp desc, avg_response_time desc

-- Regional failure correlation
fields @timestamp, region, level, @message
| filter level = "ERROR"
| stats count() as error_count by region, bin(15m)
| sort @timestamp desc, error_count desc
```

### Security Monitoring
```sql
-- Suspicious activity detection
fields @timestamp, user_id, action, ip_address, user_agent
| filter action in ["login_failed", "permission_denied", "rate_limit_exceeded"]
| stats count() as suspicious_actions,
        count_distinct(ip_address) as unique_ips
  by user_id, bin(1h)
| filter suspicious_actions > 10 or unique_ips > 5
| sort suspicious_actions desc

-- Anomalous traffic patterns
fields @timestamp, ip_address, endpoint, status_code
| filter ispresent(ip_address)
| stats count() as request_count,
        count_distinct(endpoint) as unique_endpoints,
        count(status_code >= 400) as error_requests
  by ip_address, bin(10m)
| eval error_rate = error_requests / request_count * 100
| filter request_count > 1000 or error_rate > 50
| sort request_count desc
```

## ‚û°Ô∏è Next Steps
Now that you've mastered advanced CloudWatch queries and log analysis, proceed to [10. Log-Analysis-Practice.md](10.%20Log-Analysis-Practice.md) to apply these techniques in real-world scenarios and practice comprehensive troubleshooting workflows.