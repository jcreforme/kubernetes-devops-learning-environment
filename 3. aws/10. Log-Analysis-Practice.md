# 10. Log Analysis Practice & Real-World Scenarios

## üéØ Learning Objectives
- [ ] Apply advanced log analysis techniques to real-world problems
- [ ] Master end-to-end troubleshooting workflows
- [ ] Build comprehensive monitoring and alerting systems
- [ ] Practice incident response and root cause analysis
- [ ] Create operational runbooks and playbooks
- [ ] Develop expertise in production log analysis

## üìã Practice Scenario Overview

### Scenario Setup
```
Practice Environment:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ E-commerce Microservices Application                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Services:                                              ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ user-service (authentication, profiles)            ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ product-service (catalog, inventory)               ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ cart-service (shopping cart, sessions)             ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ order-service (order processing, payments)         ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ notification-service (emails, SMS)                 ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ recommendation-service (ML-based suggestions)      ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ gateway-service (API gateway, load balancing)      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Infrastructure:                                        ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ AWS EKS Cluster (3 availability zones)             ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ RDS PostgreSQL (primary/replica)                   ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ ElastiCache Redis (sessions, cache)                ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ S3 (static assets, file uploads)                   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ CloudWatch Logs (centralized logging)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Log Group Structure
```
CloudWatch Log Groups:
/aws/eks/ecommerce-cluster/
‚îú‚îÄ‚îÄ /application/user-service
‚îú‚îÄ‚îÄ /application/product-service  
‚îú‚îÄ‚îÄ /application/cart-service
‚îú‚îÄ‚îÄ /application/order-service
‚îú‚îÄ‚îÄ /application/notification-service
‚îú‚îÄ‚îÄ /application/recommendation-service
‚îú‚îÄ‚îÄ /application/gateway-service
‚îú‚îÄ‚îÄ /infrastructure/fluent-bit
‚îî‚îÄ‚îÄ /infrastructure/kubernetes-events

Sample Log Entries:
{
  "timestamp": "2024-02-03T14:30:25.123Z",
  "level": "INFO",
  "service": "user-service",
  "traceId": "abc123def456",
  "spanId": "span789",
  "userId": "user-12345",
  "message": "User authenticated successfully",
  "duration": 45,
  "metadata": {
    "endpoint": "/auth/login",
    "method": "POST",
    "statusCode": 200
  }
}
```

## üîç Practice Exercise 1: Performance Degradation Investigation

### Scenario Description
```
Alert Received: "API response times have increased by 300% in the last 30 minutes"
Time: 2024-02-03 14:45 UTC
Affected Services: Unknown
Impact: Customer complaints about slow checkout process
```

### Step 1: Initial Investigation
```sql
-- Query 1: Overall system performance overview
fields @timestamp, service, @duration, endpoint, statusCode
| filter @timestamp >= dateadd(minute, -60, now())
| filter ispresent(@duration)
| stats avg(@duration) as avg_response_time,
        pct(@duration, 95) as p95_response_time,
        count() as request_count,
        count(statusCode >= 400) as error_count
  by service, bin(5m)
| eval error_rate = error_count / request_count * 100
| sort @timestamp desc, avg_response_time desc

-- Expected Results Analysis:
-- Look for: services with abnormally high response times
-- Correlate: timing of performance degradation
-- Identify: which services are most affected
```

### Step 2: Service-Specific Deep Dive
```sql
-- Query 2: Identify the slowest service
fields @timestamp, service, @duration, endpoint
| filter @timestamp >= dateadd(minute, -30, now())
| filter ispresent(@duration) and @duration > 5000  -- Focus on slow requests
| stats count() as slow_requests,
        avg(@duration) as avg_slow_duration,
        max(@duration) as max_duration
  by service, endpoint
| sort slow_requests desc

-- Query 3: Analyze request patterns for suspected service
fields @timestamp, @duration, endpoint, userId, traceId
| filter @timestamp >= dateadd(minute, -30, now())
| filter service = "order-service"  -- Replace with identified slow service
| sort @duration desc
| limit 20

-- Expected Investigation Results:
-- Service: order-service showing 15,000ms average response time
-- Endpoint: /api/orders/process most affected
-- Pattern: All requests to this endpoint are slow
```

### Step 3: Root Cause Analysis
```sql
-- Query 4: Database connection and query analysis
fields @timestamp, @message, service, @duration
| filter @timestamp >= dateadd(minute, -30, now())
| filter service = "order-service"
| filter @message like /database|sql|connection/
| parse @message /(?<query_type>SELECT|INSERT|UPDATE|DELETE).*duration: (?<db_duration>\d+)ms/
| stats count() as db_queries,
        avg(db_duration) as avg_db_duration,
        max(db_duration) as max_db_duration
  by query_type, bin(2m)
| sort avg_db_duration desc

-- Query 5: Connection pool analysis
fields @timestamp, @message, service
| filter service = "order-service"
| filter @message like /connection.*pool/
| parse @message /Pool: (?<pool_name>\w+) Active: (?<active>\d+) Max: (?<max>\d+) Waiting: (?<waiting>\d+)/
| eval pool_utilization = active / max * 100
| stats avg(pool_utilization) as avg_utilization,
        max(pool_utilization) as peak_utilization,
        max(waiting) as max_waiting
  by bin(1m)
| sort peak_utilization desc

-- Expected Root Cause:
-- Database connection pool exhaustion
-- Connection pool at 100% utilization
-- Long-running queries blocking connections
```

### Step 4: Impact Assessment and Resolution
```sql
-- Query 6: Business impact analysis
fields @timestamp, service, statusCode, userId, orderValue
| filter @timestamp >= dateadd(minute, -30, now())
| filter service = "order-service"
| stats count() as total_orders,
        count(statusCode >= 400) as failed_orders,
        sum(orderValue) as total_revenue_attempted,
        sum(case when statusCode < 400 then orderValue else 0 end) as successful_revenue
| eval order_failure_rate = failed_orders / total_orders * 100,
       revenue_loss = total_revenue_attempted - successful_revenue
| sort total_orders desc

-- Resolution Actions:
-- 1. Scale up database connections
-- 2. Identify and optimize slow queries
-- 3. Implement connection pooling improvements
-- 4. Add database monitoring alerts
```

## üö® Practice Exercise 2: Security Incident Response

### Scenario Description
```
Security Alert: "Unusual login patterns detected"
Time: 2024-02-03 09:15 UTC
Indicators: Multiple failed login attempts from various IPs
Concern: Potential credential stuffing attack
```

### Step 1: Attack Pattern Analysis
```sql
-- Query 1: Failed login analysis
fields @timestamp, userId, ipAddress, userAgent, action, status
| filter @timestamp >= dateadd(hour, -2, now())
| filter action = "login" and status = "failed"
| stats count() as failed_attempts,
        count_distinct(ipAddress) as unique_ips,
        count_distinct(userAgent) as unique_agents
  by userId, bin(10m)
| filter failed_attempts > 10
| sort failed_attempts desc

-- Query 2: IP address behavior analysis  
fields @timestamp, ipAddress, userId, action, status
| filter @timestamp >= dateadd(hour, -2, now())
| filter action = "login"
| stats count() as total_attempts,
        count(status="failed") as failed_attempts,
        count_distinct(userId) as targeted_users
  by ipAddress
| eval failure_rate = failed_attempts / total_attempts * 100
| filter failure_rate > 80 or targeted_users > 50
| sort targeted_users desc

-- Expected Attack Pattern:
-- 15 IP addresses targeting 500+ different usernames
-- 95%+ failure rate per IP
-- Systematic username enumeration detected
```

### Step 2: Compromise Assessment
```sql
-- Query 3: Successful breaches during attack window
fields @timestamp, userId, ipAddress, action, status, sessionId
| filter @timestamp >= dateadd(hour, -2, now())
| filter action = "login" and status = "success"
| filter ipAddress in ["192.168.1.100", "10.0.0.50"]  -- Known attack IPs
| sort @timestamp asc

-- Query 4: Post-compromise activity analysis
fields @timestamp, userId, action, ipAddress, endpoint, statusCode
| filter @timestamp >= dateadd(hour, -2, now())  
| filter userId in ["compromised-user-1", "compromised-user-2"]  -- From previous query
| sort @timestamp asc
| limit 100

-- Query 5: Lateral movement detection
fields @timestamp, userId, action, resource, permissions
| filter @timestamp >= dateadd(hour, -1, now())
| filter userId in ["compromised-user-1", "compromised-user-2"]
| filter action like /admin|delete|modify/
| stats count() as privilege_escalation_attempts by userId, action
| sort privilege_escalation_attempts desc
```

### Step 3: Response and Mitigation
```sql
-- Query 6: Account lockout verification
fields @timestamp, userId, action, status, reason
| filter @timestamp >= dateadd(minute, -30, now())
| filter action = "login" and status = "blocked"
| filter reason like /account.*locked|rate.*limit/
| stats count() as blocked_attempts by userId
| sort blocked_attempts desc

-- Query 7: Geographic analysis
fields @timestamp, ipAddress, country, city, userId, action
| filter @timestamp >= dateadd(hour, -2, now())
| filter action = "login"
| stats count() as login_attempts,
        count_distinct(userId) as unique_users
  by country, city
| sort login_attempts desc

-- Mitigation Actions Verification:
-- 1. Confirm IP blocks are effective
-- 2. Verify account lockouts are working
-- 3. Monitor for evasion attempts
-- 4. Track geographic dispersion of attacks
```

## üí• Practice Exercise 3: System Outage Investigation

### Scenario Description
```
Incident: "Complete service outage - all services returning 503"
Time: 2024-02-03 16:20 UTC
Duration: Ongoing (15 minutes)
Impact: 100% of user traffic affected
```

### Step 1: Outage Scope Analysis
```sql
-- Query 1: Service availability overview
fields @timestamp, service, statusCode, endpoint
| filter @timestamp >= dateadd(minute, -20, now())
| stats count() as total_requests,
        count(statusCode >= 500) as server_errors,
        count(statusCode = 503) as service_unavailable,
        count(statusCode < 400) as successful_requests
  by service, bin(1m)
| eval availability = successful_requests / total_requests * 100
| sort @timestamp desc, availability asc

-- Query 2: Load balancer and gateway analysis
fields @timestamp, service, @message, healthCheck, upstreamStatus
| filter service = "gateway-service"
| filter @timestamp >= dateadd(minute, -20, now())
| filter @message like /upstream|health|backend/
| parse @message /upstream (?<upstream_service>\w+) status: (?<status>\w+)/
| stats count() as health_checks,
        count(status="healthy") as healthy_checks
  by upstream_service, bin(1m)
| eval health_percentage = healthy_checks / health_checks * 100
| sort health_percentage asc
```

### Step 2: Infrastructure Investigation
```sql
-- Query 3: Kubernetes events correlation
fields @timestamp, @message, reason, involvedObject.name, involvedObject.kind
| filter @timestamp >= dateadd(minute, -25, now())
| filter @message like /Failed|Error|Warning/
| stats count() as event_count by reason, involvedObject.kind, bin(1m)
| sort event_count desc

-- Query 4: Resource exhaustion analysis
fields @timestamp, kubernetes.pod_name, @memoryUsed, cpu_percent, @message
| filter @timestamp >= dateadd(minute, -25, now())
| filter @message like /OOMKilled|memory|cpu|resource/
| stats max(@memoryUsed) as peak_memory,
        max(cpu_percent) as peak_cpu,
        count() as resource_events
  by kubernetes.pod_name
| sort peak_memory desc

-- Expected Infrastructure Issues:
-- Kubernetes cluster nodes experiencing high memory pressure
-- Multiple pods being evicted due to resource constraints
-- DNS resolution failures causing service discovery issues
```

### Step 3: Timeline Reconstruction
```sql
-- Query 5: Event timeline reconstruction
fields @timestamp, service, level, @message, kubernetes.pod_name
| filter @timestamp >= dateadd(minute, -25, now())
| filter level in ["ERROR", "FATAL", "WARN"]
| sort @timestamp asc
| limit 100

-- Query 6: Service dependency failure cascade
fields @timestamp, source_service, target_service, status, @message
| filter @timestamp >= dateadd(minute, -25, now())  
| filter @message like /connection.*refused|timeout|unavailable/
| parse @message /(?<source>\w+-service) calling (?<target>\w+-service): (?<error>.*)/
| sort @timestamp asc
| limit 50

-- Timeline Analysis Results:
-- 16:18 - Database connection pool exhaustion begins
-- 16:19 - Order service starts failing health checks
-- 16:20 - Load balancer removes order service from rotation
-- 16:20 - Cascade failure as other services depend on order service
-- 16:21 - Circuit breakers trigger across all services
```

### Step 4: Recovery Monitoring
```sql
-- Query 7: Service recovery tracking
fields @timestamp, service, statusCode, healthCheck
| filter @timestamp >= dateadd(minute, -10, now())
| filter ispresent(healthCheck)
| stats count() as total_checks,
        count(healthCheck="healthy") as healthy_checks
  by service, bin(30s)
| eval health_percentage = healthy_checks / total_checks * 100
| sort @timestamp desc

-- Query 8: Performance normalization
fields @timestamp, service, @duration, statusCode
| filter @timestamp >= dateadd(minute, -10, now())
| filter statusCode < 400
| stats avg(@duration) as avg_response_time,
        pct(@duration, 95) as p95_response_time
  by service, bin(1m)
| sort @timestamp desc

-- Recovery Validation:
-- All services showing 100% health check success
-- Response times returning to normal baselines
-- Error rates below 0.1% across all services
```

## üìä Practice Exercise 4: Business Impact Analysis

### Scenario Description
```
Business Request: "Analyze the impact of yesterday's deployment on user experience and revenue"
Deployment Time: 2024-02-02 18:00 UTC
Services Updated: recommendation-service, cart-service
Metrics Needed: Conversion rates, revenue impact, user satisfaction
```

### Step 1: User Experience Analysis
```sql
-- Query 1: User journey completion rates
fields @timestamp, userId, action, sessionId, step
| filter @timestamp >= dateadd(day, -2, now())
| filter @timestamp <= dateadd(day, -1, now())
| filter action in ["page_view", "add_to_cart", "checkout_start", "payment_complete"]
| sort @timestamp asc
| stats count() as step_count by action, bin(1h)
| eval completion_rate = step_count / lag(step_count, 1) * 100
| sort @timestamp desc

-- Query 2: Cart abandonment analysis
fields @timestamp, userId, sessionId, action, cartValue, abandonmentReason
| filter @timestamp >= dateadd(day, -2, now())
| filter action in ["add_to_cart", "cart_abandoned", "checkout_complete"]
| stats count() as cart_actions,
        count(action="add_to_cart") as carts_created,
        count(action="cart_abandoned") as carts_abandoned,
        count(action="checkout_complete") as checkouts_completed,
        sum(cartValue) as total_cart_value
  by bin(1h)
| eval abandonment_rate = carts_abandoned / carts_created * 100,
       conversion_rate = checkouts_completed / carts_created * 100
| sort @timestamp desc
```

### Step 2: Revenue Impact Assessment
```sql
-- Query 3: Revenue comparison (pre vs post deployment)
fields @timestamp, orderValue, paymentStatus, userId
| filter @timestamp >= dateadd(day, -2, now())
| filter paymentStatus = "completed"
| eval deployment_phase = if(@timestamp < "2024-02-02T18:00:00Z", "pre_deployment", "post_deployment")
| stats count() as orders,
        sum(orderValue) as total_revenue,
        avg(orderValue) as avg_order_value,
        count_distinct(userId) as unique_customers
  by deployment_phase, bin(1h)
| sort @timestamp desc

-- Query 4: Product recommendation effectiveness
fields @timestamp, userId, recommendationId, productId, action, revenue
| filter @timestamp >= dateadd(day, -2, now())
| filter action in ["recommendation_viewed", "recommendation_clicked", "product_purchased"]
| eval deployment_phase = if(@timestamp < "2024-02-02T18:00:00Z", "pre_deployment", "post_deployment")
| stats count() as recommendation_interactions,
        count(action="recommendation_clicked") as clicks,
        count(action="product_purchased") as purchases,
        sum(revenue) as recommendation_revenue
  by deployment_phase, bin(2h)
| eval click_through_rate = clicks / recommendation_interactions * 100,
       conversion_rate = purchases / clicks * 100
| sort @timestamp desc
```

### Step 3: Performance Impact on User Experience
```sql
-- Query 5: Page load time impact analysis
fields @timestamp, @duration, endpoint, userId, browserType
| filter @timestamp >= dateadd(day, -2, now())
| filter endpoint in ["/cart", "/recommendations", "/checkout"]
| eval deployment_phase = if(@timestamp < "2024-02-02T18:00:00Z", "pre_deployment", "post_deployment")
| stats avg(@duration) as avg_load_time,
        pct(@duration, 95) as p95_load_time,
        count(@duration > 3000) as slow_loads,
        count() as total_loads
  by endpoint, deployment_phase, bin(2h)
| eval slow_load_rate = slow_loads / total_loads * 100
| sort @timestamp desc

-- Query 6: Error rate impact on user flows
fields @timestamp, statusCode, endpoint, userId, sessionId
| filter @timestamp >= dateadd(day, -2, now())
| filter endpoint in ["/api/cart/*", "/api/recommendations/*"]
| eval deployment_phase = if(@timestamp < "2024-02-02T18:00:00Z", "pre_deployment", "post_deployment")
| stats count() as total_requests,
        count(statusCode >= 400) as error_requests
  by endpoint, deployment_phase, bin(1h)
| eval error_rate = error_requests / total_requests * 100
| sort @timestamp desc
```

### Step 4: Long-term Trend Analysis
```sql
-- Query 7: Customer retention impact
fields @timestamp, userId, action, sessionDuration
| filter @timestamp >= dateadd(day, -7, now())
| filter action = "session_end"
| eval deployment_phase = if(@timestamp < "2024-02-02T18:00:00Z", "pre_deployment", "post_deployment"),
       day_of_week = dayofweek(@timestamp)
| stats avg(sessionDuration) as avg_session_duration,
        count_distinct(userId) as active_users,
        count() as sessions
  by deployment_phase, day_of_week
| eval sessions_per_user = sessions / active_users
| sort day_of_week asc

-- Business Impact Summary Expected Results:
-- Pre-deployment: 3.2% conversion rate, $125 avg order value
-- Post-deployment: 3.8% conversion rate, $135 avg order value
-- Recommendation CTR: Improved from 2.1% to 3.4%
-- Cart abandonment: Reduced from 68% to 62%
-- Revenue lift: +18% in first 24 hours post-deployment
```

## üîß Practice Exercise 5: Operational Runbook Development

### Creating Investigation Playbooks
```sql
-- Playbook 1: High Error Rate Investigation
-- Step 1: Identify affected services
fields @timestamp, service, level
| filter @timestamp >= dateadd(minute, -15, now())
| filter level = "ERROR"
| stats count() as error_count by service
| sort error_count desc
| limit 5

-- Step 2: Analyze error patterns
fields @timestamp, service, @message, level
| filter @timestamp >= dateadd(minute, -15, now())
| filter level = "ERROR"
| filter service = "${TOP_ERROR_SERVICE}"  -- Replace with result from step 1
| stats count() as occurrence by @message
| sort occurrence desc
| limit 10

-- Step 3: Check service dependencies
fields @timestamp, source_service, target_service, status, @message
| filter @timestamp >= dateadd(minute, -15, now())
| filter source_service = "${TOP_ERROR_SERVICE}"
| filter @message like /timeout|connection.*failed|service.*unavailable/
| stats count() as failure_count by target_service
| sort failure_count desc
```

### Automated Alert Queries
```sql
-- Alert Query 1: Critical Error Rate
fields @timestamp, service, level
| filter @timestamp >= dateadd(minute, -5, now())
| stats count() as total_logs, count(level="ERROR") as error_logs by service
| eval error_rate = error_logs / total_logs * 100
| filter error_rate > 15  -- Alert threshold
| sort error_rate desc

-- Alert Query 2: Response Time Degradation
fields @timestamp, service, @duration
| filter @timestamp >= dateadd(minute, -10, now())
| filter ispresent(@duration)
| stats pct(@duration, 95) as current_p95 by service
| filter current_p95 > 5000  -- 5 second threshold
| sort current_p95 desc

-- Alert Query 3: Service Availability
fields @timestamp, service, statusCode
| filter @timestamp >= dateadd(minute, -5, now())
| stats count() as total_requests, 
        count(statusCode < 400) as successful_requests 
  by service
| eval availability = successful_requests / total_requests * 100
| filter availability < 99.5  -- 99.5% SLA threshold
| sort availability asc
```

## üìà Advanced Scenarios

### Multi-Cloud Log Correlation
```sql
-- Scenario: Application spans AWS and Azure
-- Azure logs forwarded to CloudWatch via custom solution

fields @timestamp, cloud_provider, service, level, @message, region
| filter @timestamp >= dateadd(hour, -1, now())
| filter level = "ERROR"
| stats count() as error_count by cloud_provider, region, service, bin(10m)
| sort error_count desc

-- Cross-cloud latency analysis
fields @timestamp, source_cloud, target_cloud, @duration, operation
| filter ispresent(source_cloud) and ispresent(target_cloud)
| filter source_cloud != target_cloud
| stats avg(@duration) as avg_cross_cloud_latency,
        pct(@duration, 95) as p95_cross_cloud_latency
  by source_cloud, target_cloud, bin(15m)
| sort avg_cross_cloud_latency desc
```

### Machine Learning Integration
```sql
-- Anomaly detection using statistical methods
fields @timestamp, service, @duration
| filter @timestamp >= dateadd(day, -7, now())
| filter ispresent(@duration)
| stats avg(@duration) as avg_duration,
        stddev(@duration) as stddev_duration
  by service, bin(1h)
| eval upper_bound = avg_duration + (2 * stddev_duration),
       lower_bound = avg_duration - (2 * stddev_duration)
| filter avg_duration > upper_bound
| sort avg_duration desc

-- Predictive failure analysis
fields @timestamp, service, cpu_percent, memory_percent, error_count
| filter @timestamp >= dateadd(hour, -6, now())
| sort @timestamp asc
| stats avg(cpu_percent) as avg_cpu,
        avg(memory_percent) as avg_memory,
        sum(error_count) as total_errors
  by service, bin(15m)
| eval failure_risk_score = (avg_cpu * 0.3) + (avg_memory * 0.4) + (total_errors * 0.3)
| filter failure_risk_score > 80
| sort failure_risk_score desc
```

## üìä Performance Benchmarking

### Query Performance Optimization
```bash
# Measure query performance
aws logs start-query \
    --log-group-name "/aws/eks/ecommerce-cluster/application" \
    --start-time $(date -d '1 hour ago' +%s) \
    --end-time $(date +%s) \
    --query-string 'fields @timestamp, service, level | filter level = "ERROR" | stats count() by service'

# Monitor query costs
aws logs describe-queries \
    --log-group-name "/aws/eks/ecommerce-cluster/application" \
    --max-results 50

# Optimization techniques demonstrated:
# 1. Time range limitation
# 2. Early filtering
# 3. Field selection optimization
# 4. Efficient aggregation patterns
```

### Cost Analysis Queries
```sql
-- Data scanned analysis
fields @timestamp, @message
| filter @timestamp >= dateadd(hour, -24, now())
| stats count() as log_entries by bin(1h)
| eval estimated_size_mb = log_entries * 0.5  -- Rough estimate
| sort @timestamp desc

-- Query efficiency measurement
fields @timestamp, service, level
| filter @timestamp >= dateadd(hour, -1, now())  -- Narrow time window
| filter service = "user-service"                 -- Specific service filter
| filter level in ["ERROR", "WARN"]              -- Specific log levels
| stats count() by level                          -- Simple aggregation
| limit 10                                       -- Result limitation
```

## üéØ Real-World Integration

### CI/CD Pipeline Integration
```yaml
# GitHub Actions workflow for log analysis
name: Post-Deployment Log Analysis
on:
  deployment_status:
    
jobs:
  log-analysis:
    runs-on: ubuntu-latest
    steps:
    - name: Analyze Deployment Impact
      run: |
        # Run CloudWatch Insights queries to analyze deployment
        DEPLOYMENT_TIME=$(date -Iseconds)
        
        # Error rate analysis
        aws logs start-query \
          --log-group-name "/aws/eks/production/application" \
          --start-time $(date -d '30 minutes ago' +%s) \
          --end-time $(date +%s) \
          --query-string '
            fields @timestamp, service, level
            | filter @timestamp >= dateadd(minute, -30, now())
            | stats count() as total, count(level="ERROR") as errors by service
            | eval error_rate = errors / total * 100
            | filter error_rate > 5
            | sort error_rate desc'
```

### Slack Integration for Alerts
```python
# Python script for automated alerting
import boto3
import requests
import json
from datetime import datetime, timedelta

def check_error_rates():
    logs_client = boto3.client('logs')
    
    query = """
    fields @timestamp, service, level
    | filter @timestamp >= dateadd(minute, -15, now())
    | stats count() as total, count(level="ERROR") as errors by service
    | eval error_rate = errors / total * 100
    | filter error_rate > 10
    | sort error_rate desc
    """
    
    end_time = datetime.now()
    start_time = end_time - timedelta(minutes=15)
    
    response = logs_client.start_query(
        logGroupName='/aws/eks/production/application',
        startTime=int(start_time.timestamp()),
        endTime=int(end_time.timestamp()),
        queryString=query
    )
    
    # Process results and send to Slack if issues found
    query_id = response['queryId']
    
    # Wait for query completion and process results
    # Send Slack notification if error rate exceeds threshold

def lambda_handler(event, context):
    check_error_rates()
    return {'statusCode': 200}
```

## üìö Knowledge Assessment

### Self-Assessment Checklist
```
Technical Skills Mastery:
‚ñ° Can write complex CloudWatch Insights queries from memory
‚ñ° Understand log aggregation and statistical analysis
‚ñ° Can correlate logs across multiple services and timeframes
‚ñ° Master error pattern recognition and root cause analysis
‚ñ° Understand performance optimization for both queries and costs

Operational Skills:
‚ñ° Can respond effectively to production incidents
‚ñ° Build comprehensive monitoring and alerting systems
‚ñ° Create operational runbooks and investigation playbooks
‚ñ° Perform business impact analysis from technical logs
‚ñ° Integrate log analysis into CI/CD and operational workflows

Advanced Techniques:
‚ñ° Multi-service correlation and dependency analysis
‚ñ° Security incident investigation and response
‚ñ° Predictive analysis and anomaly detection
‚ñ° Cost optimization and query performance tuning
‚ñ° Cross-platform and multi-cloud log analysis
```

### Certification-Style Questions
```
1. What is the most efficient way to find error patterns across multiple services?
   A) Query each service individually
   B) Use regex parsing with early filtering
   C) Extract all fields then filter
   D) Use nested subqueries

2. For cost optimization, which technique is most effective?
   A) Use broad time ranges for comprehensive analysis
   B) Always include all available fields
   C) Filter early and limit time ranges
   D) Use complex nested aggregations

3. When investigating a performance issue, what's the best first step?
   A) Check individual service logs
   B) Analyze database performance
   C) Review overall system performance trends
   D) Examine network latency

Answers: 1-B, 2-C, 3-C
```

## üéì Final Project

### Comprehensive Monitoring System
Build a complete monitoring and alerting system including:

1. **Performance Dashboard Queries**
   - Service health overview
   - Response time trends
   - Error rate monitoring
   - Business metric tracking

2. **Automated Alert System**
   - Critical error rate alerts
   - Performance degradation detection
   - Security incident triggers
   - Business impact notifications

3. **Investigation Runbooks**
   - Step-by-step troubleshooting guides
   - Query templates for common issues
   - Escalation procedures
   - Recovery validation checklists

4. **Business Reporting**
   - Daily operational reports
   - Weekly performance summaries
   - Monthly trend analysis
   - Quarterly business impact reviews

## üèÜ Congratulations!

You have completed the comprehensive AWS CloudWatch log analysis training program. You now possess the skills to:

- **Master CloudWatch Insights** queries for complex log analysis
- **Investigate and resolve** production incidents effectively  
- **Build comprehensive monitoring** and alerting systems
- **Perform business impact** analysis from technical logs
- **Optimize query performance** and manage costs
- **Create operational excellence** through systematic log analysis

## ‚û°Ô∏è Continuous Learning

### Next Steps for Advanced Mastery:
1. **Explore AWS X-Ray** for distributed tracing integration
2. **Learn Amazon OpenSearch** for advanced analytics
3. **Study AWS Config** for compliance and configuration monitoring
4. **Practice with AWS Systems Manager** for operational insights
5. **Integrate with AWS Security Hub** for security monitoring
6. **Develop custom CloudWatch** metrics and dashboards

### Community and Resources:
- AWS CloudWatch documentation and best practices
- AWS re:Invent sessions on observability
- Community-driven monitoring patterns and templates
- Open source tools and integrations
- Professional AWS certification paths

**You are now equipped to handle enterprise-scale log analysis and monitoring challenges with confidence and expertise!**