# 3. Modify Helm Values and Upgrade

## üéØ Learning Objectives
- [ ] Understand Helm upgrade workflow and best practices
- [ ] Modify Helm values for configuration changes
- [ ] Perform safe upgrades with rollback capability
- [ ] Handle version management and release history
- [ ] Implement blue-green and rolling upgrade strategies

## üìã Prerequisites
- Application deployed using Helm from previous exercises
- Multiple environments (development, production)
- kubectl and Helm CLI access
- Understanding of Helm values hierarchy

## üîÑ Step 1: Understanding Current Release State

### Examine Current Release
```bash
# List all Helm releases
helm list --all-namespaces

# Get detailed release information
helm status my-app-dev --namespace development
helm status my-app-prod --namespace production

# View current values
helm get values my-app-dev --namespace development
helm get values my-app-prod --namespace production --all

# Check release history
helm history my-app-dev --namespace development
helm history my-app-prod --namespace production
```

### Current Values Analysis
```bash
# Compare current values with defaults
helm show values ./my-app > default-values.yaml
helm get values my-app-dev -n development > current-dev-values.yaml

# Show differences
diff default-values.yaml current-dev-values.yaml
```

## üìù Step 2: Prepare Value Updates

### Create Updated Development Values
```yaml
# values-dev-v2.yaml
replicaCount: 2  # Increased from 1

image:
  repository: my-app
  tag: "dev-v1.1.0"  # New version
  pullPolicy: IfNotPresent

app:
  name: my-sample-app
  environment: development
  logLevel: info  # Changed from debug
  database:
    host: postgres-dev.default.svc.cluster.local
    port: 5432
    name: myapp_dev
    # New database settings
    connectionPool:
      maxConnections: 10
      minConnections: 2
      idleTimeout: 300

# New feature flags
featureFlags:
  enableCaching: true
  enableMetrics: true
  enableTracing: false
  newUIEnabled: true

# Updated resource allocation
resources:
  requests:
    cpu: 200m  # Increased
    memory: 256Mi  # Increased
  limits:
    cpu: 500m
    memory: 512Mi

# New monitoring configuration
monitoring:
  enabled: true
  scrapeInterval: 15s
  metricsPath: /metrics
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"

ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    # New annotation for caching
    nginx.ingress.kubernetes.io/proxy-cache-valid: "200 1h"
  hosts:
    - host: my-app-dev.company.local
      paths:
        - path: /
          pathType: Prefix

# New health check configuration
healthCheck:
  livenessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 45  # Increased
    periodSeconds: 15
    timeoutSeconds: 5
    failureThreshold: 3
  readinessProbe:
    httpGet:
      path: /ready
      port: http
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 2
```

### Create Updated Production Values
```yaml
# values-prod-v2.yaml
replicaCount: 5  # Increased from 3

image:
  repository: my-app
  tag: "v1.3.0"  # New stable version
  pullPolicy: IfNotPresent

app:
  name: my-sample-app
  environment: production
  logLevel: warn
  database:
    host: postgres-prod.database.svc.cluster.local
    port: 5432
    name: myapp_prod
    # Production database tuning
    connectionPool:
      maxConnections: 50
      minConnections: 10
      idleTimeout: 600
      leakDetectionThreshold: 60000

# Production feature flags
featureFlags:
  enableCaching: true
  enableMetrics: true
  enableTracing: true
  newUIEnabled: false  # Gradual rollout
  auditLogging: true

# Production resource allocation
resources:
  requests:
    cpu: 1000m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 2Gi

# Enhanced monitoring for production
monitoring:
  enabled: true
  scrapeInterval: 10s
  metricsPath: /metrics
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
  alerts:
    enabled: true
    thresholds:
      cpuUsage: 80
      memoryUsage: 85
      errorRate: 5

# Production-grade autoscaling
autoscaling:
  enabled: true
  minReplicas: 5
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  # New scaling policies
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

# Production ingress with advanced features
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "1000"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    # Advanced caching
    nginx.ingress.kubernetes.io/proxy-cache-valid: "200 2h"
    nginx.ingress.kubernetes.io/proxy-cache-key: "$scheme$request_method$host$request_uri"
  hosts:
    - host: my-app.company.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: my-app-tls
      hosts:
        - my-app.company.com

# Production security and compliance
security:
  podSecurityPolicy:
    enabled: true
  networkPolicy:
    enabled: true
    ingress:
      - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
  serviceAccount:
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/my-app-role

# Enhanced health checks for production
healthCheck:
  livenessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3
  readinessProbe:
    httpGet:
      path: /ready
      port: http
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 2
  startupProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30

# Affinity rules for production
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app.kubernetes.io/name
          operator: In
          values:
          - my-app
      topologyKey: kubernetes.io/hostname
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: node-type
          operator: In
          values:
          - production
```

## üöÄ Step 3: Validate Changes Before Upgrade

### Dry-Run Upgrade
```bash
# Test development upgrade
helm upgrade my-app-dev ./my-app \
  --namespace development \
  --values ./values-dev-v2.yaml \
  --dry-run \
  --debug

# Test production upgrade
helm upgrade my-app-prod ./my-app \
  --namespace production \
  --values ./values-prod-v2.yaml \
  --dry-run \
  --debug
```

### Generate and Inspect Diff
```bash
# Install helm-diff plugin if not available
helm plugin install https://github.com/databus23/helm-diff

# Show differences for development
helm diff upgrade my-app-dev ./my-app \
  --namespace development \
  --values ./values-dev-v2.yaml \
  --allow-unreleased

# Show differences for production
helm diff upgrade my-app-prod ./my-app \
  --namespace production \
  --values ./values-prod-v2.yaml \
  --allow-unreleased
```

### Template Validation
```bash
# Generate templates and validate
helm template my-app-dev ./my-app \
  --namespace development \
  --values ./values-dev-v2.yaml \
  --validate > dev-v2-manifests.yaml

helm template my-app-prod ./my-app \
  --namespace production \
  --values ./values-prod-v2.yaml \
  --validate > prod-v2-manifests.yaml

# Check for any validation errors
kubectl apply --dry-run=server -f dev-v2-manifests.yaml
kubectl apply --dry-run=server -f prod-v2-manifests.yaml
```

## üîÑ Step 4: Perform Development Upgrade

### Safe Development Upgrade
```bash
# Backup current release values
helm get values my-app-dev -n development > backup-dev-values.yaml

# Perform upgrade with safety measures
helm upgrade my-app-dev ./my-app \
  --namespace development \
  --values ./values-dev-v2.yaml \
  --wait \
  --timeout 600s \
  --history-max 5 \
  --description "Upgrade to v1.1.0 with enhanced monitoring and caching"

# Check upgrade status
helm status my-app-dev --namespace development

# Verify the upgrade
kubectl get all -n development -l app.kubernetes.io/name=my-app
```

### Monitor Development Upgrade
```bash
# Watch pods during upgrade
kubectl get pods -n development -l app.kubernetes.io/name=my-app --watch

# Check rollout status
kubectl rollout status deployment/my-app-dev -n development

# Verify new configuration is applied
kubectl get deployment my-app-dev -n development -o yaml | grep -A 10 -B 5 "image:\|replicas:"
```

### Test Upgraded Application
```bash
# Port forward to test
kubectl port-forward -n development svc/my-app-dev 8080:80 &

# Test health endpoints
curl http://localhost:8080/health
curl http://localhost:8080/ready

# Test new metrics endpoint (if enabled)
curl http://localhost:8080/metrics

# Test new features
curl -H "Feature-Flag: newUIEnabled" http://localhost:8080/

# Stop port forward
kill %1
```

## üè≠ Step 5: Production Upgrade Strategy

### Pre-Production Checklist
```bash
# Create production upgrade checklist
cat << 'EOF' > prod-upgrade-checklist.md
# Production Upgrade Checklist

## Pre-Upgrade
- [ ] Development upgrade completed and tested
- [ ] Backup current production release
- [ ] Maintenance window scheduled
- [ ] Monitoring and alerting systems ready
- [ ] Rollback plan prepared
- [ ] Team notified

## During Upgrade
- [ ] Upgrade executed during maintenance window
- [ ] Real-time monitoring active
- [ ] Health checks validated
- [ ] Performance metrics within acceptable range
- [ ] No critical alerts triggered

## Post-Upgrade
- [ ] All pods running and healthy
- [ ] Application functionality verified
- [ ] Performance metrics stable
- [ ] User acceptance testing passed
- [ ] Documentation updated
- [ ] Team notified of completion
EOF

echo "üìã Production upgrade checklist created"
```

### Staged Production Upgrade
```bash
# Method 1: Blue-Green Deployment Simulation
# Create a temporary release for testing
helm install my-app-prod-staging ./my-app \
  --namespace production \
  --values ./values-prod-v2.yaml \
  --set fullnameOverride=my-app-staging \
  --set service.port=8080 \
  --wait \
  --timeout 600s

# Test the staging deployment
kubectl port-forward -n production svc/my-app-staging 8081:8080 &

# Run tests against staging
for endpoint in health ready metrics; do
  echo "Testing /$endpoint..."
  curl -f http://localhost:8081/$endpoint
done

# If tests pass, proceed with production upgrade
kill %1

# Clean up staging
helm uninstall my-app-prod-staging --namespace production
```

### Execute Production Upgrade
```bash
# Backup current production state
helm get values my-app-prod -n production --all > backup-prod-values-$(date +%Y%m%d-%H%M%S).yaml

# Execute production upgrade with maximum safety
helm upgrade my-app-prod ./my-app \
  --namespace production \
  --values ./values-prod-v2.yaml \
  --wait \
  --timeout 900s \
  --history-max 5 \
  --atomic \
  --description "Production upgrade to v1.3.0 with enhanced scaling and monitoring" \
  --set-string "deploymentAnnotations.upgrade\.timestamp=$(date -Iseconds)"

echo "‚úÖ Production upgrade completed"
```

### Monitor Production Upgrade
```bash
# Real-time monitoring during upgrade
kubectl get pods -n production -l app.kubernetes.io/name=my-app --watch &
WATCH_PID=$!

# Check HPA scaling
kubectl get hpa -n production my-app-prod-hpa

# Monitor resource utilization
kubectl top pods -n production -l app.kubernetes.io/name=my-app

# Stop the watch when satisfied
kill $WATCH_PID
```

## üß™ Step 6: Post-Upgrade Validation

### Comprehensive Health Check
```bash
# Create post-upgrade validation script
cat << 'EOF' > validate-upgrade.sh
#!/bin/bash

NAMESPACE=${1:-production}
RELEASE_NAME=${2:-my-app-prod}

echo "üîç Validating upgrade for $RELEASE_NAME in $NAMESPACE"

# 1. Check Helm release status
echo "1. Helm release status:"
helm status $RELEASE_NAME --namespace $NAMESPACE

# 2. Check all pods are running
echo "2. Pod status:"
kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=my-app

# 3. Check services
echo "3. Service status:"
kubectl get svc -n $NAMESPACE -l app.kubernetes.io/name=my-app

# 4. Check ingress
echo "4. Ingress status:"
kubectl get ingress -n $NAMESPACE

# 5. Test health endpoints
echo "5. Health check:"
POD_NAME=$(kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=my-app -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n $NAMESPACE $POD_NAME -- curl -s http://localhost:8080/health

# 6. Check HPA (if enabled)
if kubectl get hpa -n $NAMESPACE 2>/dev/null; then
  echo "6. HPA status:"
  kubectl get hpa -n $NAMESPACE
fi

# 7. Check recent events
echo "7. Recent events:"
kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' | tail -10

echo "‚úÖ Validation complete"
EOF

chmod +x validate-upgrade.sh

# Run validation
./validate-upgrade.sh production my-app-prod
./validate-upgrade.sh development my-app-dev
```

### Performance Baseline Comparison
```bash
# Before/after performance comparison
cat << 'EOF' > performance-check.sh
#!/bin/bash

NAMESPACE=${1:-production}
DURATION=${2:-60}

echo "üìä Performance check for $DURATION seconds in $NAMESPACE"

# Test load and measure response times
kubectl run load-test --image=busybox --rm -i --restart=Never -- \
  sh -c "
    for i in \$(seq 1 100); do
      time wget -qO- http://my-app-service.$NAMESPACE.svc.cluster.local/health
      sleep 0.5
    done
  " 2>&1 | grep real

# Check resource utilization during load
kubectl top pods -n $NAMESPACE -l app.kubernetes.io/name=my-app

echo "‚úÖ Performance check complete"
EOF

chmod +x performance-check.sh
./performance-check.sh production 30
```

## üîô Step 7: Rollback Procedures

### Rollback Preparation
```bash
# Check rollback options
helm history my-app-prod --namespace production

# Show what rollback would change
helm rollback my-app-prod 1 --namespace production --dry-run
```

### Execute Rollback (If Needed)
```bash
# Immediate rollback to previous version
helm rollback my-app-prod --namespace production --wait --timeout 600s

# Rollback to specific revision
# helm rollback my-app-prod 2 --namespace production --wait --timeout 600s

# Verify rollback
helm status my-app-prod --namespace production
kubectl get pods -n production -l app.kubernetes.io/name=my-app
```

### Automated Rollback Triggers
```bash
# Create automated rollback script based on health checks
cat << 'EOF' > auto-rollback.sh
#!/bin/bash

NAMESPACE=${1:-production}
RELEASE_NAME=${2:-my-app-prod}
HEALTH_ENDPOINT=${3:-http://my-app-service.$NAMESPACE.svc.cluster.local/health}

echo "üîç Monitoring upgrade for auto-rollback triggers"

# Monitor for 5 minutes after upgrade
for i in {1..30}; do
  echo "Health check $i/30..."
  
  # Check if pods are ready
  READY_PODS=$(kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=my-app -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
  TOTAL_PODS=$(kubectl get pods -n $NAMESPACE -l app.kubernetes.io/name=my-app | grep -c my-app)
  
  # Check health endpoint
  HEALTH_STATUS=$(kubectl run health-check --image=busybox --rm -i --restart=Never -- \
    wget -qO- --timeout=5 $HEALTH_ENDPOINT 2>/dev/null || echo "FAILED")
  
  echo "Pods: $READY_PODS/$TOTAL_PODS ready, Health: $HEALTH_STATUS"
  
  # Trigger rollback if conditions met
  if [ "$READY_PODS" -lt "$((TOTAL_PODS / 2))" ] || [ "$HEALTH_STATUS" = "FAILED" ]; then
    echo "‚ùå Rollback triggered! Pods or health check failing"
    helm rollback $RELEASE_NAME --namespace $NAMESPACE --wait
    echo "üîÑ Rollback completed"
    exit 1
  fi
  
  sleep 10
done

echo "‚úÖ No rollback needed - upgrade stable"
EOF

chmod +x auto-rollback.sh

# Use this script immediately after production upgrades
# ./auto-rollback.sh production my-app-prod
```

## üìä Step 8: Advanced Upgrade Patterns

### Canary Deployment with Helm
```bash
# Create canary release with small percentage of traffic
helm install my-app-prod-canary ./my-app \
  --namespace production \
  --values ./values-prod-v2.yaml \
  --set fullnameOverride=my-app-canary \
  --set replicaCount=1 \
  --set service.port=8081 \
  --set ingress.hosts[0].host=canary-my-app.company.com \
  --wait

# Monitor canary metrics
kubectl get pods -n production -l app.kubernetes.io/name=my-app-canary

# If canary is successful, upgrade main release
# helm upgrade my-app-prod ./my-app --namespace production --values ./values-prod-v2.yaml

# Clean up canary
# helm uninstall my-app-prod-canary --namespace production
```

### Progressive Upgrade with Pauses
```bash
# Upgrade with manual approval gates
cat << 'EOF' > progressive-upgrade.sh
#!/bin/bash

NAMESPACE=${1:-production}
RELEASE_NAME=${2:-my-app-prod}

echo "üöÄ Starting progressive upgrade for $RELEASE_NAME"

# Stage 1: Upgrade with reduced replica count
echo "Stage 1: Upgrading with reduced replicas..."
helm upgrade $RELEASE_NAME ./my-app \
  --namespace $NAMESPACE \
  --values ./values-prod-v2.yaml \
  --set replicaCount=2 \
  --wait

echo "‚úã Stage 1 complete. Press Enter to continue to Stage 2..."
read

# Stage 2: Scale to 50% of target replicas
echo "Stage 2: Scaling to 50% capacity..."
helm upgrade $RELEASE_NAME ./my-app \
  --namespace $NAMESPACE \
  --values ./values-prod-v2.yaml \
  --set replicaCount=3 \
  --wait

echo "‚úã Stage 2 complete. Press Enter to continue to full deployment..."
read

# Stage 3: Full deployment
echo "Stage 3: Full deployment..."
helm upgrade $RELEASE_NAME ./my-app \
  --namespace $NAMESPACE \
  --values ./values-prod-v2.yaml \
  --wait

echo "‚úÖ Progressive upgrade complete!"
EOF

chmod +x progressive-upgrade.sh
# ./progressive-upgrade.sh production my-app-prod
```

## ‚úÖ Success Criteria

### ‚úÖ Upgrade Completion Checklist
- [ ] Development upgrade completed successfully
- [ ] Production upgrade completed without service interruption
- [ ] All pods running and healthy post-upgrade
- [ ] Health checks passing consistently
- [ ] New features/configurations active
- [ ] Performance metrics within acceptable range
- [ ] HPA scaling working correctly (production)
- [ ] Rollback capability tested and confirmed
- [ ] Monitoring and alerting functional

### ‚úÖ Validation Commands
```bash
# All these should succeed:
helm status my-app-dev -n development
helm status my-app-prod -n production
kubectl get pods -n development -l app.kubernetes.io/name=my-app
kubectl get pods -n production -l app.kubernetes.io/name=my-app
helm history my-app-prod -n production
kubectl get hpa -n production  # Should show updated settings
```

## üìä Upgrade Impact Analysis

### Measure Upgrade Success
```bash
# Create upgrade impact report
cat << 'EOF' > upgrade-report.sh
#!/bin/bash

echo "üìä Helm Upgrade Impact Report"
echo "=============================="

# Release information
echo "Release Status:"
helm list --all-namespaces

# Historical comparison
echo "Release History:"
helm history my-app-prod -n production

# Resource utilization comparison
echo "Resource Utilization:"
kubectl top pods -n production -l app.kubernetes.io/name=my-app

# Configuration changes summary
echo "Key Configuration Changes:"
echo "- Replica count: 3 ‚Üí 5"
echo "- Image version: v1.2.3 ‚Üí v1.3.0"
echo "- Resource requests: 500m/512Mi ‚Üí 1000m/1Gi"
echo "- New features: caching, enhanced monitoring"
echo "- HPA max replicas: 10 ‚Üí 20"

# Performance impact
echo "Performance Metrics:"
kubectl exec -n production deployment/my-app-prod -- curl -s http://localhost:8080/metrics | grep -E "(http_requests|cpu_usage|memory_usage)"
EOF

chmod +x upgrade-report.sh
./upgrade-report.sh
```

## üéì Key Takeaways

### What You've Learned
1. **Helm Upgrade Workflow**: Safe upgrade procedures with validation and rollback
2. **Value Management**: Systematic approach to configuration changes
3. **Production Safety**: Blue-green, canary, and progressive upgrade strategies
4. **Monitoring Integration**: Real-time upgrade monitoring and automated rollback triggers
5. **Release Management**: History tracking, backup, and recovery procedures

### Best Practices Implemented
- Always use `--dry-run` and `--diff` before actual upgrades
- Implement automated validation and rollback triggers
- Stage upgrades through development before production
- Maintain detailed release history and documentation
- Monitor key metrics during and after upgrades

This exercise demonstrates enterprise-grade Helm upgrade management, ensuring safe and reliable application updates across multiple environments.

## ‚û°Ô∏è Next Steps
Proceed to [4. Use kustomize for environment-specific configs](4.%20Use-Kustomize-for-Environment-Specific-Configs.md) to learn how to integrate Kustomize with your existing Helm deployments for enhanced configuration management.